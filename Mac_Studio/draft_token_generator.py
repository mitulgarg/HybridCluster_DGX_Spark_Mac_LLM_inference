# drafter_client.py
# Usage: python drafter_client.py --dgx http://127.0.0.1:8000 --model distilgpt2 --draft_n 4 --max_tokens 100

import argparse
import time
import requests
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

parser = argparse.ArgumentParser()
parser.add_argument("--dgx", default="http://127.0.0.1:8000", help="DGX verifier server URL")
parser.add_argument("--model", default="distilgpt2", help="Drafter model to use locally (small for CPU)")
parser.add_argument("--draft_n", type=int, default=4, help="Speculative window size (draft tokens per batch)")
parser.add_argument("--max_tokens", type=int, default=100, help="Max total tokens including prompt")
parser.add_argument("--prompt", type=str, default="Explain quantum entanglement simply.", help="Prompt")
args = parser.parse_args()

DGX_URL = args.dgx
DRAFT_N = args.draft_n
MAX_TOKENS = args.max_tokens
PROMPT = args.prompt

print(f"Loading drafter model {args.model} ...")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoModelForCausalLM.from_pretrained(args.model).to(device)
tokenizer = AutoTokenizer.from_pretrained(args.model)

# 1) Send prefill request to DGX to create KV cache on server
print("Sending prefill to DGX ...")
r = requests.post(f"{DGX_URL}/prefill", json={"prompt": PROMPT})
if r.status_code != 200:
    print("Prefill failed:", r.text)
    raise SystemExit(1)
prefill_resp = r.json()
prompt_len = prefill_resp.get("prompt_len", None)
print(f"DGX prefill ready. Prompt length = {prompt_len}")

# 2) Initialize local token list (we will keep a local copy of committed tokens)
tokens = tokenizer.encode(PROMPT)
committed = len(tokens)



print("Starting speculative decode loop ...")
start_time = time.time()

while len(tokens) < MAX_TOKENS:
    # Draft up to DRAFT_N tokens using the local drafter model
    input_ids = torch.tensor([tokens]).to(device)
    # generate returns sequences with appended tokens
    # generate with deterministic sampling may be desired; default will sample if top_k etc set.
    out = model.generate(input_ids, max_new_tokens=DRAFT_N, do_sample=True, top_p=0.95)
    generated = out[0].cpu().tolist()[len(tokens):]  # list of token ids (draft)
    if len(generated) == 0:
        print("\nNo more tokens generated by drafter. Stopping.")
        break

    # Send draft tokens to DGX for verification
    payload = {"draft_tokens": generated}
    send_time = time.time()
    r = requests.post(f"{DGX_URL}/verify", json=payload)
    if r.status_code != 200:
        print("Verify failed:", r.text)
        break
    resp = r.json()
    accepted = resp.get("accepted_prefix_len", 0)

    # Commit accepted prefix
    if accepted > 0:
        tokens.extend(generated[:accepted])
        # print committed text
        committed_text = tokenizer.decode(tokens[committed:])
        print(committed_text, end="", flush=True)
        committed = len(tokens)
    else:
        # nothing accepted: handle rollback or reduce draft size / temperature; here we just retry once with smaller draft
        print("\nNo tokens accepted; retrying with draft_n=1 once.")
        # simple fallback: generate one token greedily
        out2 = model.generate(input_ids, max_new_tokens=1, do_sample=False)
        gen1 = out2[0].cpu().tolist()[len(tokens):]
        if not gen1:
            print("Drafter could not produce a token. Stopping.")
            break
        r2 = requests.post(f"{DGX_URL}/verify", json={"draft_tokens": gen1})
        resp2 = r2.json()
        accepted2 = resp2.get("accepted_prefix_len", 0)
        if accepted2 > 0:
            tokens.extend(gen1[:accepted2])
            committed_text = tokenizer.decode(tokens[committed:])
            print(committed_text, end="", flush=True)
            committed = len(tokens)
        else:
            print("Still not accepted. Aborting.")
            break

    # small throttle to avoid hotloop on CPU
    time.sleep(0.05)

end_time = time.time()
print("\n\nGeneration finished. Total tokens:", len(tokens), "Duration:", end_time - start_time, "s")
print("Final output:\n", tokenizer.decode(tokens))
